import pandas as pd
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class TrafficDataCleaner:
    def __init__(self):
        self.cleaning_log = []
        self.original_count = 0
        self.cleaned_count = 0
        
    def log_step(self, step_name, removed_count, remaining_count):
        """è®°å½•æ¸…æ´—æ­¥éª¤"""
        self.cleaning_log.append({
            'step': step_name,
            'removed': removed_count,
            'remaining': remaining_count,
            'removal_rate': f"{removed_count/self.original_count*100:.2f}%"
        })
        print(f"âœ“ {step_name}: ç§»é™¤ {removed_count:,} æ¡è®°å½•, å‰©ä½™ {remaining_count:,} æ¡ ({removed_count/self.original_count*100:.2f}%)")
    
    def clean_traffic_data(self, df, gentle_mode=False):
        """
        å®Œæ•´çš„äº¤é€šæ•°æ®æ¸…æ´—æµç¨‹
        
        å‚æ•°:
        gentle_mode (bool): Trueä¸ºæ¸©å’Œæ¨¡å¼ï¼ŒFalseä¸ºæ ‡å‡†æ¨¡å¼
        """
        print("ğŸš— å¼€å§‹äº¤é€šæ•°æ®æ¸…æ´—...")
        if gentle_mode:
            print("ğŸŒ± ä½¿ç”¨æ¸©å’Œæ¸…æ´—æ¨¡å¼")
        else:
            print("âš¡ ä½¿ç”¨æ ‡å‡†æ¸…æ´—æ¨¡å¼")
        print("=" * 60)
        
        # å¤‡ä»½åŸå§‹æ•°æ®
        original_df = df.copy()
        self.original_count = len(df)
        print(f"ğŸ“Š åŸå§‹æ•°æ®é‡: {self.original_count:,} æ¡è®°å½•")
        print()
        
        # ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¸…æ´—
        print("ğŸ”§ ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¸…æ´—")
        print("-" * 30)
        if gentle_mode:
            df = self._remove_extreme_outliers_gentle(df)
        else:
            df = self._remove_extreme_outliers(df)
        df = self._physical_constraints_check(df)
        print()
        
        # ç¬¬äºŒé˜¶æ®µï¼šé€»è¾‘ä¸€è‡´æ€§æ¸…æ´—
        print("ğŸ”§ ç¬¬äºŒé˜¶æ®µï¼šé€»è¾‘ä¸€è‡´æ€§æ¸…æ´—")
        print("-" * 30)
        if gentle_mode:
            df = self._traffic_flow_relationship_check_gentle(df)
        else:
            df = self._traffic_flow_relationship_check(df)
        df = self._traffic_theory_constraints(df)
        print()
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šæ—¶é—´è¿ç»­æ€§æ¸…æ´—
        print("ğŸ”§ ç¬¬ä¸‰é˜¶æ®µï¼šæ—¶é—´è¿ç»­æ€§æ¸…æ´—")
        print("-" * 30)
        df = self._remove_insufficient_data_segments(df)
        if gentle_mode:
            df = self._smooth_time_series_outliers_gentle(df)
        else:
            df = self._smooth_time_series_outliers(df)
        print()
        
        # ç¬¬å››é˜¶æ®µï¼šç»Ÿè®¡æ¸…æ´—
        print("ğŸ”§ ç¬¬å››é˜¶æ®µï¼šç»Ÿè®¡æ¸…æ´—")
        print("-" * 30)
        if gentle_mode:
            df = self._percentile_truncation_gentle(df)
        else:
            df = self._percentile_truncation(df)
        df = self._zscore_cleaning(df)
        print()
        
        self.cleaned_count = len(df)
        self._print_cleaning_summary(original_df, df)
        
        return df
    
    def _remove_extreme_outliers_gentle(self, df):
        """ç§»é™¤IQRæç«¯å¼‚å¸¸å€¼ï¼ˆæ¸©å’Œç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # ä½¿ç”¨æ›´å®½æ¾çš„4å€IQRèŒƒå›´ï¼Œåªç§»é™¤çœŸæ­£æç«¯çš„å¼‚å¸¸å€¼
        combined_mask = True
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 4 * IQR  # ä»3å€æ”¹ä¸º4å€ï¼Œæ›´å®½æ¾
            upper_bound = Q3 + 4 * IQR
            
            col_mask = (df[col] >= lower_bound) & (df[col] <= upper_bound)
            combined_mask = combined_mask & col_mask
        
        df_cleaned = df[combined_mask].copy()
        
        removed_count = initial_count - len(df_cleaned)
        self.log_step("ç§»é™¤IQRæç«¯å¼‚å¸¸å€¼(æ¸©å’Œ)", removed_count, len(df_cleaned))

        return df_cleaned
        
    def _remove_extreme_outliers(self, df):
        """ç§»é™¤IQRæç«¯å¼‚å¸¸å€¼ï¼ˆæ ‡å‡†ç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # ä½¿ç”¨3å€IQRèŒƒå›´
        combined_mask = True
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 3 * IQR
            upper_bound = Q3 + 3 * IQR
            
            col_mask = (df[col] >= lower_bound) & (df[col] <= upper_bound)
            combined_mask = combined_mask & col_mask
        
        df_cleaned = df[combined_mask].copy()
        
        removed_count = initial_count - len(df_cleaned)
        self.log_step("ç§»é™¤IQRæç«¯å¼‚å¸¸å€¼", removed_count, len(df_cleaned))

        return df_cleaned
        
    def _traffic_flow_relationship_check(self, df):
        """äº¤é€šæµåŸºæœ¬å…³ç³»éªŒè¯: Flow = Speed Ã— Densityï¼ˆæ ‡å‡†ç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # è®¡ç®—ç†è®ºæµé‡
        theoretical_flow = df['SPEED_kph'] * df['DENSITY_vpkm']
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        relative_error = np.abs(df['FLOW_vph'] - theoretical_flow) / np.maximum(df['FLOW_vph'], 1)
        
        # ä¿ç•™è¯¯å·®å°äº15%çš„è®°å½•
        flow_consistency_mask = relative_error <= 0.15
        
        df_cleaned = df[flow_consistency_mask].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("äº¤é€šæµåŸºæœ¬å…³ç³»éªŒè¯", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _smooth_time_series_outliers(self, df):
        """æ—¶é—´åºåˆ—å¼‚å¸¸å¹³æ»‘å¤„ç†ï¼ˆæ ‡å‡†ç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # æŒ‰è·¯æ®µå’Œæ—¥æœŸæ’åº
        df_sorted = df.sort_values(['ROADSECT_ID', 'DATE', 'TIME']).copy()
        
        # è®¡ç®—ç›¸é‚»æ—¶é—´ç‚¹çš„å˜åŒ–ç‡
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            # æŒ‰è·¯æ®µåˆ†ç»„è®¡ç®—å˜åŒ–ç‡
            df_sorted[f'{col}_change_rate'] = df_sorted.groupby('ROADSECT_ID')[col].pct_change().abs()
        
        # æ ‡è®°å˜åŒ–ç‡è¿‡å¤§çš„ç‚¹
        high_change_mask = (
            (df_sorted['SPEED_kph_change_rate'] > 0.8) |
            (df_sorted['FLOW_vph_change_rate'] > 1.0) |
            (df_sorted['DENSITY_vpkm_change_rate'] > 1.0)
        )
        
        # ç§»é™¤çªå˜ç‚¹
        df_cleaned = df_sorted[~high_change_mask.fillna(False)].copy()
        
        # æ¸…ç†ä¸´æ—¶åˆ—
        change_cols = [col for col in df_cleaned.columns if col.endswith('_change_rate')]
        df_cleaned = df_cleaned.drop(columns=change_cols)
        
        removed_count = initial_count - len(df_cleaned)
        self.log_step("æ—¶é—´åºåˆ—çªå˜å¤„ç†", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _percentile_truncation(self, df):
        """åˆ†ä½æ•°æˆªæ–­ï¼ˆæ ‡å‡†ç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # ä½¿ç”¨1%-99%åˆ†ä½æ•°æˆªæ–­
        percentile_mask = True
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            p1 = df[col].quantile(0.01)
            p99 = df[col].quantile(0.99)
            percentile_mask = percentile_mask & (df[col] >= p1) & (df[col] <= p99)
        
        df_cleaned = df[percentile_mask].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("åˆ†ä½æ•°æˆªæ–­(1%-99%)", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _traffic_flow_relationship_check_gentle(self, df):
        """äº¤é€šæµåŸºæœ¬å…³ç³»éªŒè¯: Flow = Speed Ã— Densityï¼ˆæ¸©å’Œç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # è®¡ç®—ç†è®ºæµé‡
        theoretical_flow = df['SPEED_kph'] * df['DENSITY_vpkm']
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®ï¼Œä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
        relative_error = np.abs(df['FLOW_vph'] - theoretical_flow) / np.maximum(df['FLOW_vph'], 1)
        
        # ä¿ç•™è¯¯å·®å°äº25%çš„è®°å½•ï¼ˆæ¯”æ ‡å‡†ç‰ˆçš„15%æ›´å®½æ¾ï¼‰
        flow_consistency_mask = relative_error <= 0.25
        
        df_cleaned = df[flow_consistency_mask].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("äº¤é€šæµåŸºæœ¬å…³ç³»éªŒè¯(æ¸©å’Œ)", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _smooth_time_series_outliers_gentle(self, df):
        """æ—¶é—´åºåˆ—å¼‚å¸¸å¹³æ»‘å¤„ç†ï¼ˆæ¸©å’Œç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # æŒ‰è·¯æ®µå’Œæ—¥æœŸæ’åº
        df_sorted = df.sort_values(['ROADSECT_ID', 'DATE', 'TIME']).copy()
        
        # è®¡ç®—ç›¸é‚»æ—¶é—´ç‚¹çš„å˜åŒ–ç‡ï¼Œä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            # æŒ‰è·¯æ®µåˆ†ç»„è®¡ç®—å˜åŒ–ç‡
            df_sorted[f'{col}_change_rate'] = df_sorted.groupby('ROADSECT_ID')[col].pct_change().abs()
        
        # ä½¿ç”¨æ›´å®½æ¾çš„å˜åŒ–ç‡é˜ˆå€¼ï¼Œåªç§»é™¤æç«¯çªå˜
        high_change_mask = (
            (df_sorted['SPEED_kph_change_rate'] > 1.5) |  # ä»0.8æ”¹ä¸º1.5
            (df_sorted['FLOW_vph_change_rate'] > 2.0) |   # ä»1.0æ”¹ä¸º2.0
            (df_sorted['DENSITY_vpkm_change_rate'] > 2.0)  # ä»1.0æ”¹ä¸º2.0
        )
        
        # ç§»é™¤çªå˜ç‚¹
        df_cleaned = df_sorted[~high_change_mask.fillna(False)].copy()
        
        # æ¸…ç†ä¸´æ—¶åˆ—
        change_cols = [col for col in df_cleaned.columns if col.endswith('_change_rate')]
        df_cleaned = df_cleaned.drop(columns=change_cols)
        
        removed_count = initial_count - len(df_cleaned)
        self.log_step("æ—¶é—´åºåˆ—çªå˜å¤„ç†(æ¸©å’Œ)", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _percentile_truncation_gentle(self, df):
        """åˆ†ä½æ•°æˆªæ–­ï¼ˆæ¸©å’Œç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # ä½¿ç”¨0.5%-99.5%åˆ†ä½æ•°æˆªæ–­ï¼Œæ¯”1%-99%æ›´å®½æ¾
        percentile_mask = True
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            p05 = df[col].quantile(0.005)  # 0.5%åˆ†ä½æ•°
            p995 = df[col].quantile(0.995)  # 99.5%åˆ†ä½æ•°
            percentile_mask = percentile_mask & (df[col] >= p05) & (df[col] <= p995)
        
        df_cleaned = df[percentile_mask].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("åˆ†ä½æ•°æˆªæ–­(0.5%-99.5%)", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _physical_constraints_check(self, df):
        """ç‰©ç†çº¦æŸæ£€æŸ¥"""
        initial_count = len(df)
        
        # ç‰©ç†åˆç†æ€§æ£€æŸ¥
        physical_mask = (
            (df['SPEED_kph'] > 0) & (df['SPEED_kph'] <= 120) &  # é€Ÿåº¦åˆç†èŒƒå›´
            (df['FLOW_vph'] >= 0) & (df['FLOW_vph'] <= 3000) &  # æµé‡åˆç†èŒƒå›´
            (df['DENSITY_vpkm'] > 0) & (df['DENSITY_vpkm'] <= 200)  # å¯†åº¦åˆç†èŒƒå›´
        )
        
        df_cleaned = df[physical_mask].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("ç‰©ç†çº¦æŸæ£€æŸ¥", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _traffic_flow_relationship_check(self, df):
        """äº¤é€šæµåŸºæœ¬å…³ç³»éªŒè¯: Flow = Speed Ã— Density"""
        initial_count = len(df)
        
        # è®¡ç®—ç†è®ºæµé‡
        theoretical_flow = df['SPEED_kph'] * df['DENSITY_vpkm']
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        relative_error = np.abs(df['FLOW_vph'] - theoretical_flow) / np.maximum(df['FLOW_vph'], 1)
        
        # ä¿ç•™è¯¯å·®å°äº15%çš„è®°å½•ï¼ˆæ¯”æŠ¥å‘Šä¸­çš„10%ç¨å®½æ¾ï¼‰
        flow_consistency_mask = relative_error <= 0.15
        
        df_cleaned = df[flow_consistency_mask].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("äº¤é€šæµåŸºæœ¬å…³ç³»éªŒè¯", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _traffic_theory_constraints(self, df):
        """äº¤é€šæµç†è®ºçº¦æŸ"""
        initial_count = len(df)
        
        # ç§»é™¤è¿èƒŒäº¤é€šæµç†è®ºçš„è®°å½•
        theory_mask = ~(
            (df['DENSITY_vpkm'] < 10) & (df['SPEED_kph'] < 20) |  # ä½å¯†åº¦ä¸åº”ä½é€Ÿ
            (df['DENSITY_vpkm'] > 80) & (df['SPEED_kph'] > 40) |  # é«˜å¯†åº¦ä¸åº”é«˜é€Ÿ
            (df['FLOW_vph'] > 2000) & (df['SPEED_kph'] < 10)      # é«˜æµé‡ä¸åº”æä½é€Ÿ
        )
        
        df_cleaned = df[theory_mask].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("äº¤é€šæµç†è®ºçº¦æŸ", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _remove_insufficient_data_segments(self, df):
        """ç§»é™¤æ•°æ®é‡ä¸è¶³çš„è·¯æ®µ"""
        initial_count = len(df)
        
        # è®¡ç®—æ¯ä¸ªè·¯æ®µçš„æ•°æ®é‡
        segment_counts = df.groupby('ROADSECT_ID').size()
        sufficient_segments = segment_counts[segment_counts >= 100].index
        
        df_cleaned = df[df['ROADSECT_ID'].isin(sufficient_segments)].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("ç§»é™¤æ•°æ®ä¸è¶³è·¯æ®µ", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _smooth_time_series_outliers(self, df):
        """æ—¶é—´åºåˆ—å¼‚å¸¸å¹³æ»‘å¤„ç†ï¼ˆæ¸©å’Œç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # æŒ‰è·¯æ®µå’Œæ—¥æœŸæ’åº
        df_sorted = df.sort_values(['ROADSECT_ID', 'DATE', 'TIME']).copy()
        
        # è®¡ç®—ç›¸é‚»æ—¶é—´ç‚¹çš„å˜åŒ–ç‡ï¼Œä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            # æŒ‰è·¯æ®µåˆ†ç»„è®¡ç®—å˜åŒ–ç‡
            df_sorted[f'{col}_change_rate'] = df_sorted.groupby('ROADSECT_ID')[col].pct_change().abs()
        
        # ä½¿ç”¨æ›´å®½æ¾çš„å˜åŒ–ç‡é˜ˆå€¼ï¼Œåªç§»é™¤æç«¯çªå˜
        high_change_mask = (
            (df_sorted['SPEED_kph_change_rate'] > 1.5) |  # ä»0.8æ”¹ä¸º1.5
            (df_sorted['FLOW_vph_change_rate'] > 2.0) |   # ä»1.0æ”¹ä¸º2.0
            (df_sorted['DENSITY_vpkm_change_rate'] > 2.0)  # ä»1.0æ”¹ä¸º2.0
        )
        
        # ç§»é™¤çªå˜ç‚¹
        df_cleaned = df_sorted[~high_change_mask.fillna(False)].copy()
        
        # æ¸…ç†ä¸´æ—¶åˆ—
        change_cols = [col for col in df_cleaned.columns if col.endswith('_change_rate')]
        df_cleaned = df_cleaned.drop(columns=change_cols)
        
        removed_count = initial_count - len(df_cleaned)
        self.log_step("æ—¶é—´åºåˆ—çªå˜å¤„ç†(æ¸©å’Œ)", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _percentile_truncation(self, df):
        """åˆ†ä½æ•°æˆªæ–­ï¼ˆæ¸©å’Œç‰ˆæœ¬ï¼‰"""
        initial_count = len(df)
        
        # ä½¿ç”¨0.5%-99.5%åˆ†ä½æ•°æˆªæ–­ï¼Œæ¯”1%-99%æ›´å®½æ¾
        percentile_mask = True
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            p05 = df[col].quantile(0.005)  # 0.5%åˆ†ä½æ•°
            p995 = df[col].quantile(0.995)  # 99.5%åˆ†ä½æ•°
            percentile_mask = percentile_mask & (df[col] >= p05) & (df[col] <= p995)
        
        df_cleaned = df[percentile_mask].copy()
        removed_count = initial_count - len(df_cleaned)
        self.log_step("åˆ†ä½æ•°æˆªæ–­(0.5%-99.5%)", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _zscore_cleaning(self, df):
        """Z-scoreæ¸…æ´—"""
        initial_count = len(df)
        
        # æŒ‰è·¯æ®µè®¡ç®—Z-score
        zscore_mask = True
        
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            # è®¡ç®—æ¯ä¸ªè·¯æ®µçš„Z-score
            df[f'{col}_zscore'] = df.groupby('ROADSECT_ID')[col].transform(
                lambda x: np.abs(stats.zscore(x, nan_policy='omit'))
            )
            # ä¿ç•™Z-score < 3.5çš„æ•°æ®
            zscore_mask = zscore_mask & (df[f'{col}_zscore'] < 3.5)
        
        df_cleaned = df[zscore_mask].copy()
        
        # æ¸…ç†ä¸´æ—¶åˆ—
        zscore_cols = [col for col in df_cleaned.columns if col.endswith('_zscore')]
        df_cleaned = df_cleaned.drop(columns=zscore_cols)
        
        removed_count = initial_count - len(df_cleaned)
        self.log_step("Z-scoreæ¸…æ´—", removed_count, len(df_cleaned))
        
        return df_cleaned
    
    def _print_cleaning_summary(self, original_df, cleaned_df):
        """æ‰“å°æ¸…æ´—æ€»ç»“"""
        print("ğŸ“‹ æ•°æ®æ¸…æ´—æ€»ç»“æŠ¥å‘Š")
        print("=" * 60)
        
        total_removed = self.original_count - self.cleaned_count
        removal_rate = total_removed / self.original_count * 100
        
        print(f"åŸå§‹æ•°æ®é‡: {self.original_count:,} æ¡")
        print(f"æ¸…æ´—åæ•°æ®é‡: {self.cleaned_count:,} æ¡")
        print(f"æ€»ç§»é™¤æ•°æ®: {total_removed:,} æ¡ ({removal_rate:.2f}%)")
        print(f"æ•°æ®ä¿ç•™ç‡: {100-removal_rate:.2f}%")
        print()
        
        print("ğŸ“Š æ¸…æ´—å‰åæ•°æ®è´¨é‡å¯¹æ¯”:")
        print("-" * 40)
        
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            print(f"\n{col}:")
            print(f"  æ¸…æ´—å‰: å‡å€¼={original_df[col].mean():.2f}, æ ‡å‡†å·®={original_df[col].std():.2f}")
            print(f"  æ¸…æ´—å: å‡å€¼={cleaned_df[col].mean():.2f}, æ ‡å‡†å·®={cleaned_df[col].std():.2f}")
            
            # è®¡ç®—ååº¦å’Œå³°åº¦æ”¹å–„
            original_skew = original_df[col].skew()
            cleaned_skew = cleaned_df[col].skew()
            original_kurt = original_df[col].kurtosis()
            cleaned_kurt = cleaned_df[col].kurtosis()
            
            print(f"  ååº¦æ”¹å–„: {original_skew:.3f} â†’ {cleaned_skew:.3f}")
            print(f"  å³°åº¦æ”¹å–„: {original_kurt:.3f} â†’ {cleaned_kurt:.3f}")
        
        print("\nğŸ¯ å¼‚å¸¸å€¼æ£€æµ‹ç»“æœ:")
        print("-" * 40)
        for col in ['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm']:
            Q1 = cleaned_df[col].quantile(0.25)
            Q3 = cleaned_df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = ((cleaned_df[col] < lower_bound) | (cleaned_df[col] > upper_bound)).sum()
            outlier_rate = outliers / len(cleaned_df) * 100
            print(f"  {col} å¼‚å¸¸å€¼æ¯”ä¾‹: {outlier_rate:.2f}% ({outliers:,} æ¡)")

def main(gentle_mode=True):
    """
    ä¸»å‡½æ•°ç¤ºä¾‹
    
    å‚æ•°:
    gentle_mode (bool): Trueä¸ºæ¸©å’Œæ¨¡å¼ï¼ˆæ¨èï¼‰ï¼ŒFalseä¸ºæ ‡å‡†æ¨¡å¼
    """
    # åˆ›å»ºæ¸…æ´—å™¨å®ä¾‹
    cleaner = TrafficDataCleaner()
    
    # è¯»å–æ•°æ®ï¼ˆè¯·æ ¹æ®å®é™…æ–‡ä»¶è·¯å¾„ä¿®æ”¹ï¼‰
    print("ğŸ“– æ­£åœ¨è¯»å–æ•°æ®...")
    df = pd.read_csv(r'E:\github_projects\math_modeling_playground\playground\è·¯æ®µé€Ÿåº¦æ•°æ®_03.csv')  # è¯·æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    
    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    df['SPEED_kph'] = pd.to_numeric(df['SPEED_kph'], errors='coerce')
    df['FLOW_vph'] = pd.to_numeric(df['FLOW_vph'], errors='coerce')
    df['DENSITY_vpkm'] = pd.to_numeric(df['DENSITY_vpkm'], errors='coerce')
    
    # ç§»é™¤ç¼ºå¤±å€¼
    df = df.dropna(subset=['SPEED_kph', 'FLOW_vph', 'DENSITY_vpkm'])
    
    # æ‰§è¡Œæ¸…æ´—
    cleaned_df = cleaner.clean_traffic_data(df, gentle_mode=gentle_mode)
    
    # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    mode_suffix = "_gentle" if gentle_mode else "_standard"
    output_file = f'cleaned_traffic_data{mode_suffix}.csv'
    cleaned_df.to_csv(output_file, index=False)
    print(f"\nğŸ’¾ æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜è‡³: {output_file}")
    
    return cleaned_df

if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹ - æ¨èä½¿ç”¨æ¸©å’Œæ¨¡å¼
    cleaned_data = main(gentle_mode=True)
    
    # å¦‚æœä½ å·²ç»æœ‰DataFrameï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š
    # cleaner = TrafficDataCleaner()
    # cleaned_df = cleaner.clean_traffic_data(your_dataframe, gentle_mode=True)
    
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("- gentle_mode=True: æ¸©å’Œæ¸…æ´—ï¼Œä¿ç•™æ›´å¤šæ•°æ®ï¼ˆæ¨èï¼‰")
    print("- gentle_mode=False: æ ‡å‡†æ¸…æ´—ï¼Œæ•°æ®è´¨é‡æ›´é«˜ä½†å¯èƒ½æŸå¤±è¾ƒå¤šæ•°æ®")
    pass